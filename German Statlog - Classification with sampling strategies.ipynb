{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as sklearn_lm\n",
    "import matplotlib.pyplot as mp_plt\n",
    "import seaborn as sns\n",
    "import sklearn.decomposition as sklearn_dcmp\n",
    "import sklearn.preprocessing as sklearn_pproc\n",
    "import imblearn.over_sampling as imblearn_osmpl\n",
    "import sklearn.metrics as sklearn_mt\n",
    "import pdb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and test set\n",
    "def split_train_test(data, NUM_SAMPLES, dr=0.8):\n",
    "  NUM_TRAIN = int(NUM_SAMPLES * dr) # dr = data ratio\n",
    "  NUM_TEST = NUM_SAMPLES - NUM_TRAIN\n",
    "#   pdb.set_trace()\n",
    "  train_indexes = np.random.choice(NUM_SAMPLES, NUM_TRAIN, replace=False)\n",
    "  test_indexes = np.setdiff1d(np.arange(0,NUM_SAMPLES), train_indexes, assume_unique=True)\n",
    "  train_data = data[train_indexes]\n",
    "  test_data = data[test_indexes]\n",
    "  \n",
    "  X_train = train_data[:,:-1]\n",
    "  y_train = train_data[:,-1]\n",
    "  X_test = test_data[:,:-1]\n",
    "  y_test = test_data[:,-1]\n",
    "  \n",
    "  return {'train':[NUM_TRAIN, X_train, y_train], 'test':[NUM_TEST, X_test, y_test]}\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('german.txt', delimiter=',', header=None)\n",
    "data = data.values\n",
    "NUM_SAMPLES = data.shape[0]\n",
    "\n",
    "# define constants\n",
    "NUM_CLASSES = 2\n",
    "NUM_ITER = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr(X):\n",
    "  # compute covariance matrix\n",
    "  X_cov = np.dot(X.T, X) / X.shape[0]\n",
    "  \n",
    "  mp_plt.figure(figsize=(8,6))\n",
    "  ax = mp_plt.gca()\n",
    "  sns.heatmap(X_cov, xticklabels=np.arange(1,25), yticklabels=np.arange(1,25))\n",
    "  ax.set_title('Feature Correlation')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ROC curve and ROC area for each class\n",
    "def make_roc(y_test, y_score):  \n",
    "  y_test = 2- y_test\n",
    "  mp_plt.figure(figsize=(12,12))\n",
    "  mp_plt.subplots_adjust(hspace=0.5)\n",
    "  fpr, tpr, _ = sklearn_mt.roc_curve(y_test, y_score[:, 0], pos_label=1)\n",
    "  roc_auc = sklearn_mt.auc(fpr, tpr)\n",
    "\n",
    "  mp_plt.plot(fpr, tpr, color='darkorange', \n",
    "              linewidth=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "  mp_plt.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--')\n",
    "  mp_plt.xlim([0.0, 1.0])\n",
    "  mp_plt.ylim([0.0, 1.05])\n",
    "  mp_plt.xlabel('False Positive Rate')\n",
    "  mp_plt.ylabel('True Positive Rate')\n",
    "  mp_plt.title('Receiver operating characteristic for Class: {} (Good Credit Score)'.format(1))\n",
    "  mp_plt.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make boxplots\n",
    "def make_boxplots(acc, recall, precision):\n",
    "  mp_plt.figure(figsize=(12,12))\n",
    "  mp_plt.subplots_adjust(wspace=0.5)\n",
    "  mp_plt.subplot(1,3,1)\n",
    "  mp_plt.boxplot(acc)\n",
    "  mp_plt.ylim(acc.min()-0.05, acc.max()+0.05)\n",
    "  mp_plt.title('Boxplot for accuracy')\n",
    "  mp_plt.subplot(1,3,2)\n",
    "  mp_plt.boxplot(recall)\n",
    "  mp_plt.ylim(acc.min()-0.05, acc.max()+0.05)\n",
    "  mp_plt.title('Boxplot for recall')\n",
    "  mp_plt.subplot(1,3,3)\n",
    "  mp_plt.boxplot(precision)\n",
    "  mp_plt.ylim(acc.min()-0.05, acc.max()+0.05)\n",
    "  mp_plt.title('Boxplot for precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7215833333333334 0.0014390208333333332\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(y_test, y_pred):\n",
    "  return np.float64(sum(y_test == y_pred)) / np.float64(y_test.size)\n",
    "\n",
    "def linear_classifier(data, NUM_SAMPLES, NUM_ITER):\n",
    "  # initialise\n",
    "  acc = np.empty((NUM_ITER, 1))\n",
    "  recall = np.empty((NUM_ITER, 1))\n",
    "  precision = np.empty((NUM_ITER, 1))\n",
    "  \n",
    "  for i in range(NUM_ITER):\n",
    "    # random subsampling\n",
    "    l1_idx = np.asarray([i for i in range(NUM_SAMPLES) if data[i,-1] == 1])\n",
    "    l2_idx = np.asarray([i for i in range(NUM_SAMPLES) if data[i,-1] == 2])\n",
    "    l1_idx = np.random.choice(l1_idx, l2_idx.size, replace=False)\n",
    "    data_subsampled = np.concatenate((data[l1_idx], data[l2_idx]), axis=0)\n",
    "    np.random.shuffle(data_subsampled)\n",
    "    \n",
    "    # generate training and test data\n",
    "    data_dict = split_train_test(data_subsampled, data_subsampled.shape[0])\n",
    "    NUM_TRAIN, NUM_TEST = data_dict['train'][0], data_dict['test'][0]\n",
    "    X_train, X_test = data_dict['train'][1], data_dict['test'][1]\n",
    "    y_train, y_test = data_dict['train'][2], data_dict['test'][2]\n",
    "\n",
    "    # normalise data\n",
    "    standardscaler = sklearn_pproc.StandardScaler()\n",
    "    X_train = standardscaler.fit_transform(X_train.astype('float64'))\n",
    "    X_test = standardscaler.transform(X_test.astype('float64'))\n",
    "    \n",
    "    # data correlation\n",
    "#     get_corr(X_train)\n",
    "    \n",
    "    # dimensionality reduction\n",
    "    pca = sklearn_dcmp.PCA(n_components=11, whiten=True)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    # randomly sample data from both classes\n",
    "#     clf = sklearn_lm.SGDClassifier(loss='squared_loss', penalty='l2', max_iter=10)\n",
    "    clf = sklearn_lm.LogisticRegression(penalty='l2')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_score_pred = clf.decision_function(X_test)\n",
    "    y_score_pred = clf.predict_proba(X_test)\n",
    "    \n",
    "    # compute accuracy, recall and precision\n",
    "    y_test = 2 - y_test\n",
    "    y_pred = 2 - y_pred\n",
    "    acc[i] = sklearn_mt.accuracy_score(y_test, y_pred)\n",
    "    recall[i] = sklearn_mt.recall_score(y_test, y_pred)\n",
    "    precision[i] = sklearn_mt.precision_score(y_test, y_pred)\n",
    "\n",
    "  return [acc, recall, precision, y_test, y_score_pred]\n",
    "\n",
    "acc, recall, precision, y_test, y_score = linear_classifier(data, NUM_SAMPLES, 100)\n",
    "print(np.mean(acc), np.var(acc))\n",
    "# make_roc(y_test, y_score)\n",
    "# make_boxplots(acc, recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7444999999999999 0.0005569948979591839\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(y_test, y_pred):\n",
    "  return np.float64(sum(y_test == y_pred)) / np.float64(y_test.size)\n",
    "\n",
    "def linear_classifier(data, NUM_ITER):\n",
    "  # initialise\n",
    "  acc = np.empty((NUM_ITER, 1))\n",
    "  recall = np.empty((NUM_ITER, 1))\n",
    "  precision = np.empty((NUM_ITER, 1))\n",
    "  \n",
    "  for i in range(NUM_ITER):\n",
    "    # random oversampling\n",
    "    smote_sample = imblearn_osmpl.SMOTE(ratio='auto')\n",
    "    X_res, y_res = smote_sample.fit_sample(data[:,:-1], data[:,-1])\n",
    "    data_oversampled = np.concatenate((X_res, np.expand_dims(y_res, axis=-1)), axis=-1)\n",
    "    np.random.shuffle(data_oversampled)\n",
    "\n",
    "    # generate training and test data\n",
    "    data_dict = split_train_test(data_oversampled, data_oversampled.shape[0])\n",
    "    NUM_TRAIN, NUM_TEST = data_dict['train'][0], data_dict['test'][0]\n",
    "    X_train, X_test = data_dict['train'][1], data_dict['test'][1]\n",
    "    y_train, y_test = data_dict['train'][2], data_dict['test'][2]\n",
    "\n",
    "    # normalise data\n",
    "    standardscaler = sklearn_pproc.StandardScaler()\n",
    "    X_train = standardscaler.fit_transform(X_train)\n",
    "    X_test = standardscaler.transform(X_test)\n",
    "    \n",
    "    # data correlation\n",
    "#     get_corr(X_train)\n",
    "    \n",
    "    # dimensionality reduction\n",
    "#     pca = sklearn_dcmp.PCA(n_components=2, whiten=True)\n",
    "#     X_train = pca.fit_transform(X_train)\n",
    "#     X_test = pca.transform(X_test)\n",
    "    \n",
    "    # randomly sample data from both classes\n",
    "#     clf = sklearn_lm.SGDClassifier(loss='log', penalty='l2', max_iter=10)\n",
    "    clf = sklearn_lm.LogisticRegression(penalty='l2')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_score_pred = clf.predict_proba(X_test)\n",
    "    \n",
    "    # compute accuracy, recall and precision\n",
    "    y_test = 2 - y_test\n",
    "    y_pred = 2 - y_pred\n",
    "    acc[i] = sklearn_mt.accuracy_score(y_test, y_pred)\n",
    "    recall[i] = sklearn_mt.recall_score(y_test, y_pred)\n",
    "    precision[i] = sklearn_mt.precision_score(y_test, y_pred)\n",
    "\n",
    "  return [acc, recall, precision, y_test, y_score_pred]\n",
    "\n",
    "acc, recall, precision, y_test, y_score = linear_classifier(data, 100)\n",
    "print(np.mean(acc), np.var(acc))\n",
    "# make_roc(y_test, y_score)\n",
    "# make_boxplots(acc, recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421428571428572 0.000546173469387755\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as smd\n",
    "\n",
    "def get_accuracy(y_test, y_pred):\n",
    "  return np.float64(sum(y_test == y_pred)) / np.float64(y_test.size)\n",
    "\n",
    "def linear_classifier(data, NUM_ITER):\n",
    "  # initialise\n",
    "  acc = np.empty((NUM_ITER, 1))\n",
    "  recall = np.empty((NUM_ITER, 1))\n",
    "  precision = np.empty((NUM_ITER, 1))\n",
    "  \n",
    "  for i in range(NUM_ITER):\n",
    "    # random oversampling\n",
    "    smote_sample = imblearn_osmpl.SMOTE(ratio='auto')\n",
    "    X_res, y_res = smote_sample.fit_sample(data[:,:-1], data[:,-1])\n",
    "    data_oversampled = np.concatenate((X_res, np.expand_dims(y_res, axis=-1)), axis=-1)\n",
    "    np.random.shuffle(data_oversampled)\n",
    "\n",
    "    # generate training and test data\n",
    "    data_dict = split_train_test(data_oversampled, data_oversampled.shape[0])\n",
    "    NUM_TRAIN, NUM_TEST = data_dict['train'][0], data_dict['test'][0]\n",
    "    X_train, X_test = data_dict['train'][1], data_dict['test'][1]\n",
    "    y_train, y_test = data_dict['train'][2], data_dict['test'][2]\n",
    "\n",
    "    # normalise data\n",
    "    standardscaler = sklearn_pproc.StandardScaler()\n",
    "    X_train = standardscaler.fit_transform(X_train)\n",
    "    X_test = standardscaler.transform(X_test)\n",
    "    \n",
    "    # data correlation\n",
    "#     get_corr(X_train)\n",
    "    \n",
    "    # dimensionality reduction\n",
    "    pca = sklearn_dcmp.PCA(n_components=11, whiten=True)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    # randomly sample data from both classes\n",
    "#     clf = sklearn_lm.SGDClassifier(loss='log', penalty='l2', max_iter=10)\n",
    "    clf = sklearn_lm.LogisticRegression(penalty='l2')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # compute accuracy, recall and precision\n",
    "    y_test = 2 - y_test\n",
    "    y_pred = 2 - y_pred\n",
    "    acc[i] = sklearn_mt.accuracy_score(y_test, y_pred)\n",
    "    recall[i] = sklearn_mt.recall_score(y_test, y_pred)\n",
    "    precision[i] = sklearn_mt.precision_score(y_test, y_pred)\n",
    "\n",
    "  return [acc, recall, precision]\n",
    "\n",
    "acc, recall, precision = linear_classifier(data, 100)\n",
    "print(np.mean(acc), np.var(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
